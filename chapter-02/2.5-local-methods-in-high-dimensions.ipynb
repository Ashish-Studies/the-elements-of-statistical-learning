{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Local Methods in High Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean squared error for estimating f(0): **\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    " \\text{MSE}(x_0) & = E_\\tau[f(x_0) - \\hat{y_0}]^2\\\\\n",
    " & = E_\\tau[(f(x_0) - E_\\tau(\\hat{y_0})) + (E_\\tau(\\hat{y_0}) - \\hat{y_0})]^2\\\\\n",
    " & = E_\\tau[(E_\\tau(\\hat{y_0}) - \\hat{y_0})^2  + 2(f(x_0) - E_\\tau(\\hat{y_0}))(E_\\tau(\\hat{y_0}) - \\hat{y_0})+ (f(x_0) - E_\\tau(\\hat{y_0}))^2]\\\\\n",
    " & = E_\\tau[(E_\\tau(\\hat{y_0}) - \\hat{y_0})^2] + E_\\tau[(E_\\tau(\\hat{y_0}) - f(x_0))^2]\\\\\n",
    " & = E_\\tau[\\hat{y_0} - E_\\tau(\\hat{y_0})]^2 + [E_\\tau(\\hat{y_0}) - f(x_0)]^2\\\\\n",
    " & = Var_\\tau(\\hat{y_0}) + Bias^2(\\hat{y_0})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We have broken down the MSE into two components: variance and squared bias. Such decomposition is always possible and is known as *the bias-variance decomposition*.\n",
    "\n",
    "**(2.26) Suppose that the relationship between Y and X is linear:**\n",
    "\n",
    "$$Y = X^T\\beta + \\varepsilon $$\n",
    "\n",
    "where $\\varepsilon \\sim N(0, \\sigma^2)$ and we fit the model by least squares to the training data. For a test point x_0 we have $\\hat{y_0}=x_0^T\\hat{\\beta}$ which can be written as $\\hat{y_0} = x_0^T\\beta + \\sum_{i=1}^N {l_i(x_0)\\varepsilon_i}$ where $l_i(x_0)$ is the $i$th element of $\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}x_0$\n",
    "\n",
    "Proof:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " \\hat{\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} \\\\\n",
    " \\hat{\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\beta + \\varepsilon) \\\\\n",
    " \\hat{\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\beta + \\varepsilon) \\\\\n",
    " \\hat{\\beta}=\\beta + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\varepsilon \\\\\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "and by plugging $\\hat{B}$ into the linear model:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " \\hat{y_0} = x_0^T(\\beta+(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\varepsilon) \\\\\n",
    " \\hat{y_0} = x_0^T\\beta+x_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\varepsilon\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "we can get $\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}x_0$ from $(x_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T)^T$ by using two matrix properties:\n",
    "\n",
    "1. $(\\mathbf{AB})^T=\\mathbf{B}^T\\mathbf{A}^T$\n",
    "\n",
    "2. $(\\mathbf{A}^{-1})^T = (\\mathbf{A}^T)^{-1}$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
