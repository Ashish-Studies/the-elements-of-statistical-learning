{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Local Methods in High Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean squared error for estimating f(0):**\n",
    "\n",
    "*Assume that the relationship between X and Y is: $Y = f(X)$*\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    " \\text{MSE}(x_0) & = E_\\tau[f(x_0) - \\hat{y_0}]^2\\\\\n",
    " & = E_\\tau[(f(x_0) - E_\\tau(\\hat{y_0})) + (E_\\tau(\\hat{y_0}) - \\hat{y_0})]^2\\\\\n",
    " & = E_\\tau[(E_\\tau(\\hat{y_0}) - \\hat{y_0})^2  + 2(f(x_0) - E_\\tau(\\hat{y_0}))(E_\\tau(\\hat{y_0}) - \\hat{y_0})+ (f(x_0) - E_\\tau(\\hat{y_0}))^2]\\\\\n",
    " & = E_\\tau[(E_\\tau(\\hat{y_0}) - \\hat{y_0})^2] + E_\\tau[(E_\\tau(\\hat{y_0}) - f(x_0))^2]\\\\\n",
    " & = E_\\tau[\\hat{y_0} - E_\\tau(\\hat{y_0})]^2 + [E_\\tau(\\hat{y_0}) - f(x_0)]^2\\\\\n",
    " & = Var_\\tau(\\hat{y_0}) + Bias^2(\\hat{y_0})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We have broken down the MSE into two components: variance and squared bias. Such decomposition is always possible and is known as *the bias-variance decomposition*.\n",
    "\n",
    "*(2.26) Suppose that the relationship between Y and X is linear with some noise:*\n",
    "\n",
    "$$Y = X^T\\beta + \\varepsilon $$\n",
    "\n",
    "where $\\varepsilon \\sim N(0, \\sigma^2)$ and we fit the model by least squares to the training data. For a test point x_0 we have $\\hat{y_0}=x_0^T\\hat{\\beta}$ which can be written as $\\hat{y_0} = x_0^T\\beta + \\sum_{i=1}^N {l_i(x_0)\\varepsilon_i}$ where $l_i(x_0)$ is the $i$th element of $\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}x_0$\n",
    "\n",
    "Proof:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " \\hat{\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} \\\\\n",
    " \\hat{\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\beta + \\varepsilon) \\\\\n",
    " \\hat{\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\beta + \\varepsilon) \\\\\n",
    " \\hat{\\beta}=\\beta + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\varepsilon \\\\\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "and by plugging $\\hat{B}$ into the linear model:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " \\hat{y_0} = x_0^T(\\beta+(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\varepsilon) \\\\\n",
    " \\hat{y_0} = x_0^T\\beta+x_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\varepsilon\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "we can get $\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}x_0$ from $(x_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T)^T$ by using two matrix properties:\n",
    "\n",
    "1. $(\\mathbf{AB})^T=\\mathbf{B}^T\\mathbf{A}^T$\n",
    "\n",
    "2. $(\\mathbf{A}^{-1})^T = (\\mathbf{A}^T)^{-1}$\n",
    "\n",
    "\n",
    "\n",
    "Under this model the least square estimates are unbiased, so the expected prediction error will be:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{EPE}(x_0) & = E_{y_0|x_0}E_\\tau(y_0-\\hat{y_0})^2\\\\\n",
    "& = \\text{Var}(y_0|x_0) + Var_\\tau(\\hat{y_0}) + \\text{Bias}^2(\\hat{y_0})\\\\\n",
    "& = \\sigma^2 + E_{\\tau}x_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}x_0\\sigma^2 + 0^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "*Proof*:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{EPE}(x_0) & = E_{y_0|x_0}E_\\tau(y_0-\\hat{y_0})^2\\\\\n",
    "& = E_{y_0|x_0}E_\\tau((y_0 - f(x_0)) + (f(x_0) - \\hat{y_0}))^2\\\\\n",
    "& = E_{y_0|x_0}E_\\tau(y_0 - f(x_0))^2 + 2E_{y_0|x_0}E_\\tau(y_0 - f(x_0))(f(x_0) - \\hat{y_0}) + E_{y_0|x_0}E_\\tau(f(x_0) - \\hat{y_0})^2\\\\\n",
    "& = U_1 + U_2 + U_3\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "There are three components $U_1$, $U_2$, $U_3$ and we're going to expand them as well. \n",
    "\n",
    "$U_1 = E_{y_0|x_0}E_\\tau(y_0 - f(x_0))^2 = E_{y_0|x_0}(y_0-f(x_0))^2 = \\sigma^2$ \n",
    "\n",
    "*Note*: $f(x_0) = E_{y_0|x_0}(y_0) $\n",
    "\n",
    "\n",
    "$U_2 = 2E_{y_0|x_0}E_\\tau(y_0 - f(x_0))(f(x_0) - \\hat{y_0}) = 0$ \n",
    "\n",
    "*Note*: $E_{y_0|x_0}(y_0-f(x_0)) = 0$\n",
    "\n",
    "\n",
    "*$U_3$*:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "U_3 & = E_{y_0|x_0}E_\\tau(f(x_0) - \\hat{y_0})^2\\\\\n",
    "& = E_{y_0|x_0}E_\\tau((\\hat{y_0} - E_\\tau(\\hat{y_0})) + (E_\\tau(\\hat{y_0}) - f(x_0)))^2\\\\\n",
    "& = E_{y_0|x_0}E_\\tau(\\hat{y_0} - E_\\tau(\\hat{y_0}))^2 + 2E_{y_0|x_0}E_\\tau[(\\hat{y_0} - E_\\tau(\\hat{y_0}))(E_\\tau(\\hat{y_0}) - f(x_0))] + E_{y_0|x_0}E_\\tau(E_\\tau(\\hat{y_0}) - f(x_0))^2\\\\\n",
    "& = E_\\tau(\\hat{y_0} - E_\\tau(\\hat{y_0}))^2 + (E_\\tau(\\hat{y_0}) - f(x_0))^2\\\\\n",
    "& = \\text{Var}_\\tau(\\hat{y_0}) + \\text{Bias}_\\tau^2(\\hat{y_0}) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Finally if we sum all $U_i$ we get: $$\\text{EPE}(x_0) = U_1+U_2+U_3 = \\sigma^2 + 0 + (\\text{Var}_\\tau(\\hat{y_0}) + \\text{Bias}_\\tau^2(\\hat{y_0}))$$\n",
    "\n",
    "$E_\\tau(\\hat{y_0}) = E_\\tau(x_0^T\\beta + \\sum_{i=1}^N {l_i(x_0)\\varepsilon_i})=x_0^T\\beta + E(\\sum_{i=1}^N {l_i(x_0)\\varepsilon_i}) = x_0^T\\beta + 0$ thus $\\text{Bias}_\\tau{\\hat{y_0}} = 0$ \n",
    "\n",
    "and we can find variance: \n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Var}_\\tau(\\hat{y_0}) & = E_\\tau(\\hat{y_0} - E_\\tau(\\hat{y_0})) ^ 2\\\\\n",
    "& = E_\\tau(x_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\varepsilon)\\\\\n",
    "& = E_\\tau(x_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\varepsilon\\varepsilon^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}x_0)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\varepsilon\\varepsilon^T=\\sigma^2\\mathbf{I}_n$, so we can simplify further:\n",
    "$$\\text{Var}_\\tau(\\hat{y_0}) = \\sigma^2x_0^{T}E_\\tau[(\\mathbf{X}^T\\mathbf{X})^{-1})]x_0$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
