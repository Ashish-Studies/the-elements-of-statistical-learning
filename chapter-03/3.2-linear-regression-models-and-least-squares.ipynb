{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Linear Regression Models and Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an input vector $X^T=(X_1,...,X_p)$ and want to predict a real-valued output $Y$.The linear regression model has the form:\n",
    "\n",
    "$$f(X) = B_0 + \\sum_{j=1}^p {X_j\\beta_j}$$\n",
    "\n",
    " Typically we have a set of training data $(x_1, y_1)...(x_N, y_n)$ from which to estimate the parameters $\\beta$. The most popular estimation method is *least squares*, in which we pick $\\beta$ to minimize the residual sum of squares, (3.2):\n",
    "$$ \n",
    "\\begin{align}\n",
    "RSS(\\beta)&=\\sum_{i=1}^N(y_i-f(x_i))\\\\\n",
    "&=\\sum_{i=1}^N(y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j})^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "How do we minimize (3.2)? We can write the (3.2) using matrix, (3.3):\n",
    "$$RSS(\\beta)=(\\mathbf{y}-\\mathbf{X}\\beta)^T(\\mathbf{y}-\\mathbf{X}\\beta)$$\n",
    "\n",
    "Differentiating with respect to $\\beta$ we obtain:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial{RSS}}{\\partial\\beta} = -2\\mathbf{X}^T(\\mathbf{y}-\\mathbf{X}\\beta)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Assuming that **X** has full column rank, and hence the second derivative is positive definite:\n",
    "$$\\mathbf{X}^T(\\mathbf{y}-\\mathbf{X}\\beta)=0$$\n",
    "\n",
    "and the unique solution is:\n",
    "$$\\hat{\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "The predicted value at an input vector $x_0$ are given by $\\hat{f}(x_0)=(1:x_0)^T\\hat{\\beta}$:\n",
    "\n",
    "$$\\hat{y}=\\mathbf{X}\\hat{\\beta}=\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "The matrix $\\mathbf{H}=\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T$ is sometimes called the \"hat\" matrix.\n",
    "\n",
    "*Geometrical representation of the least squares:* We denote the column vectors of **X** by $x_0, x_1, ..., x_p$. These vectors span a subspace of $\\mathcal{R}^N$, also referred as the column space of **X**. We minimize $RSS(\\beta)=||\\mathbf{y}-\\mathbf{X}\\beta||^2$ by choosing $\\hat{\\beta}$ so that the residual vector $\\mathbf{y} - \\hat{\\mathbf{y}}$ is orthogonal to this subspace and the orthogonality is expressed by $\\mathbf{X}^T(\\mathbf{y}-\\mathbf{X}\\beta)=0$. The hat matrix **H** is the projection matrix.\n",
    "\n",
    "*Sampling properties of $\\hat{\\beta}$*: In order to pin down the sampling properties of $\\hat{\\beta}$, we assume that the observations $y_i$ are uncorrelated and have constant variance $\\sigma^2$, and that the $x_i$ are fixed. The variance-covariance matrix is given by:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Var(\\hat{\\beta}) &= E\\left[(\\hat{\\beta}-E(\\hat{\\beta}))(\\hat{\\beta}-E(\\hat{\\beta})^T)\\right]\\\\\n",
    "&= E\\left[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{\\varepsilon}\\mathbf{\\varepsilon}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\right]\\\\\n",
    "&= \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "One estimates the variance $\\sigma^2$ by:\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{N-p-1} \\sum_{i=1}^N(y_i-\\hat{y_i})^2\n",
    "$$\n",
    "\n",
    "The N-p-1 rather than N in the denominator makes $\\hat{\\sigma}^2$ an unbiased estimate of $\\sigma^2$: $E(\\hat{\\sigma}^2)=\\sigma^2$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
