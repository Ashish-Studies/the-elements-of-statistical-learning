{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4.1 Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression shrinks the regression coefficients by imposing a penalty on their size. The ridge coefficients minimize a penalized RSS (3.41):\n",
    "$$\n",
    "\\hat{\\beta}^{ridge} = \\underset{\\beta}{argmin} \\left\\{\n",
    "\\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\n",
    "+\\gamma\\sum_{j=1}^p{\\beta_j^2}\n",
    "\\right\\}\n",
    "$$\n",
    "Here $\\gamma \\ge 0$ is a complexity parameter: $\\gamma \\rightarrow \\infty$, the coefficients are shrunk toward zero (and each other). This idea also used in neural networks (known as *weight decay*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An equivalent way to write the ridge problem is (3.42):\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}^{ridge} = \\underset{\\beta}{argmin} \\left\\{\n",
    "\\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\n",
    "\\right\\},\\\\\n",
    "\\text{subject to } \\sum_{j=1}^p {\\beta_j^2 \\le t}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "There is a one-to-one correspondence between $\\gamma \\text{ and } t$. A large positive coefficient on one variable can be canceled by a similarly large negative coefficient on its correlated cousin. By imposing constraints as in (3.42), this problem is alleviated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ridge solutions are not equivariant under scaling of the inputs, so one normally standardizes the inputs before solving (3.41). \n",
    "\n",
    "Notice that the intercepts has been left out of the penalty term: because, adding a constant to each target $y_i$ would not simply result in a shift of the prediction by the same amount. It can be shown that the solution to (3.41) can be separated into two parts, after reparametrization using *centered* inputs, i.e replacing $x_{ij} \\text{ by } x_{ij}-\\overline{x_j}$:\n",
    "1. we estimate $\\beta_0 \\text{ as } \\overline{y}$,\n",
    "2. The remaining coefficients get estimated by a ridge regression.\n",
    "\n",
    "*Proof*: **TODO**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that the centering has been done, so that the matrix **X** has p columns (3.43):\n",
    "\n",
    "$$\n",
    "RSS(\\gamma)=(\\mathbf{y} - \\mathbf{X}\\beta)^T(\\mathbf{y} - \\mathbf{X}\\beta) + \\gamma\\beta^T\\beta\n",
    "$$\n",
    "\n",
    "the ridge regression solutions are easily seen to be:\n",
    "$$\n",
    "\\hat{\\beta}^{ridge}=(\\mathbf{X}^T\\mathbf{X}+\\gamma\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "$$\n",
    "\n",
    "where **I** is the $p\\times p$ identity matrix. Notice that the solution is again a linear function of **y**.\n",
    "\n",
    "*Proof*:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\cfrac{\\partial{RSS(\\gamma)}}{\\partial{\\beta}}\n",
    "&=\\cfrac{\\partial{((\\mathbf{y} - \\mathbf{X}\\beta)^T(\\mathbf{y} - \\mathbf{X}\\beta) + \\gamma\\beta^T\\beta)}}{\\partial{\\beta}}\\\\\n",
    "&=\\cfrac{\\partial{\\left(\n",
    "  \\mathbf{y}^T\\mathbf{y} - 2\\mathbf{y}^T\\mathbf{X}\\beta \n",
    "  + (\\mathbf{X}\\beta)^T\\mathbf{X}\\beta \n",
    "  + \\gamma\\beta^T\\beta \n",
    "\\right)}}{\\partial{\\beta}}\\\\\n",
    "&=-2\\mathbf{y}^T\\mathbf{X}+2\\beta^T\\mathbf{X}^T\\mathbf{X}+2\\gamma\\beta^T\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We set the first derivative to zero:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\cfrac{\\partial{RSS(\\gamma)}}{\\partial{\\beta}} = 0\\\\\n",
    "-\\mathbf{y}^T\\mathbf{X}+\\beta^T\\mathbf{X}^T\\mathbf{X}+\\gamma\\beta^T=0\\\\\n",
    "\\beta^T(\\mathbf{X}^T\\mathbf{X}+\\gamma\\mathbf{I})=\\mathbf{y}^T\\mathbf{X}\\\\\n",
    "\\beta^T=\\mathbf{y}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X}+\\gamma\\mathbf{I})^{-1}\\\\\n",
    "\\beta=(\\mathbf{X}^T\\mathbf{X}+\\gamma\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 3.8 shows the ridge coefficient estimates for the prostate cancer example, plotted as functions of $df(\\gamma)$, the *effective degrees of freedom* implied by the penalty $\\gamma$. In the case of orthonormal inputs, the ridge estimates are: $\\hat{\\beta}^{ridge}=\\hat{\\beta}/(1+\\gamma)$\n",
    "\n",
    "**TODO:** Implement Figure 3.8.\n",
    "\n",
    "Ridge expression can be also derived as the mean or mode of a posterior distribution, with a prior distribution. Suppose, $y_i \\sim N(\\beta_0+x_i^T\\beta, \\sigma^2)$ and the $\\beta_j \\sim N(0, \\mathcal{T}^2)$, independently of one another. Then the log-posterior density of $\\beta$, is equal to the expression in (3.41), with $\\gamma=\\sigma^2/\\mathcal{T}^2$.\n",
    "\n",
    "**TODO**: proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *singular value decomposition* (SVD) of the centered input matrix **X** gives us some insight into the nature of ridge regression. The SVD of the $N \\times p$ matrix **X** (3.45):\n",
    "$$\n",
    "\\mathbf{X}=\\mathbf{UD}\\mathbf{V}^T\n",
    "$$\n",
    "- **U** - $N \\times p$ orthogonal matrix, the columns of **U** span the column space of **X**.\n",
    "- **V** - $N \\times p$ orthogonal matrix, the columns of **V** span the row space of **X**.\n",
    "- **D** - $p \\times p$ diagonal matrix, with diagonal entries $d_1 \\ge d_2 \\ge ... \\ge d_p \\ge 0$, called singular values of **X**. If one or more values $d_j = 0$, **X** is singular.\n",
    "\n",
    "Using SVD we can write the least squares fitted vector as (3.46):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{X}\\hat{\\beta}^{ls} &= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\\\\n",
    "&= \\mathbf{UD}\\mathbf{V}^T(\\mathbf{VD}\\mathbf{U}^T\\mathbf{UD}\\mathbf{V}^T)^{-1}\\mathbf{VD}\\mathbf{U}^T\\mathbf{y}\\\\\n",
    "&= \\mathbf{UD}\\mathbf{V}^T(\\mathbf{VD}\\mathbf{D}\\mathbf{V}^T)^{-1}\\mathbf{VD}\\mathbf{U}^T\\mathbf{y}\\\\\n",
    "&= \\mathbf{UD}\\mathbf{V}^T[\\mathbf{V}^T]^{-1}\\mathbf{D}^{-1}\\mathbf{D}^{-1}\\mathbf{V}^{-1}\\mathbf{VD}\\mathbf{U}^T\\mathbf{y}\\\\\n",
    "&= \\mathbf{UD}\\mathbf{D}^{-1}\\mathbf{D}^{-1}\\mathbf{D}\\mathbf{U}^T\\mathbf{y}\\\\\n",
    "&= \\mathbf{U}\\mathbf{U}^T\\mathbf{y}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the ridge solution are (3.47):\n",
    "    \n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{X}\\hat{\\beta}^{ridge}&=\\mathbf{X}(\\mathbf{X}^T\\mathbf{X}+\\gamma\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\\\\\n",
    "&=\\mathbf{UD}\\mathbf{V}^T\n",
    "(\\mathbf{V}\\mathbf{D}^2\\mathbf{V}^T+\\gamma\\mathbf{V}\\mathbf{V}^T)^{-1}\n",
    "\\mathbf{VD}\\mathbf{U}^T\\mathbf{y}\\\\\n",
    "&=\\mathbf{UD}\\mathbf{V}^T\n",
    "(\\mathbf{V}(\\mathbf{D}^2+\\gamma\\mathbf{I})\\mathbf{V}^T)^{-1}\n",
    "\\mathbf{VD}\\mathbf{U}^T\\mathbf{y}\\\\\n",
    "&=\\mathbf{UD}\\mathbf{V}^T\n",
    "\\mathbf{V}(\\mathbf{D}^2+\\gamma\\mathbf{I})^{-1}\\mathbf{V}^T\n",
    "\\mathbf{VD}\\mathbf{U}^T\\mathbf{y}\\\\\n",
    "&=\\mathbf{UD}(\\mathbf{D}^2+\\gamma\\mathbf{I})^{-1}\\mathbf{D}\\mathbf{U}^T\\mathbf{y}\\\\\n",
    "&=\\sum_{j=1}^p\\mathbf{u}_j\\cfrac{d_j^2}{d_j^2+\\gamma}\\mathbf{u}_j^T\\mathbf{y}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Like linear regression, ridge regression computes the coordinates of **y** w.r.t the orthonormal basis **U**. It then shrinks these coordinates of **y** by the factors  $d_j^2/(d_j^2+\\gamma)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
